{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import detectron2\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import ColorMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_dicts(img_dir, json_file):\n",
    "    json_file = os.path.join(img_dir, json_file)\n",
    "    with open(json_file) as f:\n",
    "        imgs_anns = json.load(f)\n",
    "\n",
    "    dataset_dicts = []\n",
    "    for idx, v in enumerate(imgs_anns['images']):\n",
    "        record = {}\n",
    "        filename = os.path.join(img_dir, v[\"file_name\"])\n",
    "        \n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"id\"] = v[\"id\"]\n",
    "        record[\"height\"] = v[\"height\"]\n",
    "        record[\"width\"] = v[\"width\"]\n",
    "        \n",
    "        annos = imgs_anns[\"annotations\"]\n",
    "        objs = []\n",
    "        for anno in annos:\n",
    "            if anno['image_id'] == v[\"id\"]:\n",
    "                if anno['category_id'] == 1:\n",
    "                    obj = {\n",
    "                        \"bbox\": anno['bbox'],\n",
    "                        \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                        \"segmentation\": anno[\"segmentation\"],\n",
    "                        \"category_id\": anno['category_id'],\n",
    "                        \"iscrowd\": 0\n",
    "                    }\n",
    "                else:\n",
    "                    # Buat 4 point\n",
    "                    xywh = anno['bbox']\n",
    "                    px = [xywh[0], xywh[0]+xywh[2], xywh[0]+xywh[2], xywh[0]]\n",
    "                    py = [xywh[1], xywh[1], xywh[1]+xywh[3], xywh[1]+xywh[3]]\n",
    "                    poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "                    poly = list(itertools.chain.from_iterable(poly))\n",
    "                    obj = {\n",
    "                        \"bbox\": anno['bbox'],\n",
    "                        \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                        \"segmentation\": [poly],\n",
    "                        \"category_id\": anno['category_id'],\n",
    "                        \"iscrowd\": 0\n",
    "                    }\n",
    "                objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../RoboCup-Dataset/\"\n",
    "for d in [\"train\",\"valid\"]:\n",
    "    DatasetCatalog.register(\"robocup_\" + d, lambda d=d: get_dataset_dicts(DATASET_PATH + d, d+\".json\"))\n",
    "    MetadataCatalog.get(\"robocup_\" + d).set(thing_classes=[\"ball\", \"field\", \"left_goal\", \"right_goal\"])\n",
    "robocup_metadata = MetadataCatalog.get(\"robocup_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dicts = get_dataset_dicts(DATASET_PATH+\"train\", \"train.json\")\n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=robocup_metadata, scale=0.5)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    plt.imshow(vis.get_image()[:, :, ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.OUTPUT_DIR = \"../weights/maskrcnn_50/\"\n",
    "cfg.merge_from_file(\"../../detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml\")\n",
    "cfg.DATASETS.TRAIN = (\"robocup_train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 0\n",
    "cfg.SOLVER.IMS_PER_BATCH = 16\n",
    "cfg.SOLVER.BASE_LR = 0.0001\n",
    "cfg.SOLVER.MAX_ITER = 2000    # 300 iterations seems good enough, but you can certainly train longer\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 16\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
    "\n",
    "# Cek apakah wights sudah ada\n",
    "weights_filename = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "if os.path.exists(weights_filename):\n",
    "    print(\"Load old weights\")\n",
    "    cfg.MODEL.WEIGHTS = weights_filename\n",
    "else:\n",
    "    print(\"Download pretrained weights\")\n",
    "    cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"  # initialize from model zoo\n",
    "\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8 \n",
    "cfg.DATASETS.TEST = (\"robocup_train\")\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "dataset_dicts = get_dataset_dicts(DATASET_PATH+\"train\")\n",
    "for d in random.sample(dataset_dicts, 8):  \n",
    "    start_time = time.time()\n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=robocup_metadata, \n",
    "                   scale=0.8,\n",
    "                   instance_mode=ColorMode.IMAGE_BW\n",
    "    )\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    plt.imshow(v.get_image()[:, :, ::-1])\n",
    "    plt.show()\n",
    "    end_time = time.time()\n",
    "    print(\"FPS : \", (1/(end_time - start_time)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
